{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b0bcb0d-77c3-4f74-b4fe-ba48e7074b02",
   "metadata": {},
   "source": [
    "# Neural Nets from Scratch in Julia\n",
    "\n",
    "## Lesson 16: Softmax Activation / Crossentropy Loss\n",
    "\n",
    "* In this video we'll implement the softmax activation function and crossentropy loss function, combined into one operation.\n",
    "* [Documentation site here](https://mikesaint-antoine.github.io/SimpleGrad.jl)\n",
    "* [Github repo here](https://github.com/mikesaint-antoine/SimpleGrad.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e0de6d-3b02-422d-bb1c-2770485e459b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop! (generic function with 9 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## code so far\n",
    "\n",
    "\n",
    "struct Operation{FuncType, ArgTypes}\n",
    "    op::FuncType\n",
    "    args::ArgTypes\n",
    "end\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "###### Values\n",
    "\n",
    "mutable struct Value{opType} <: Number\n",
    "    data::Float64\n",
    "    grad::Float64\n",
    "    op::opType\n",
    "end\n",
    "\n",
    "# constructor -- Value(data, grad, op)\n",
    "Value(x::Number) = Value(Float64(x), 0.0, nothing);\n",
    "\n",
    "\n",
    "import Base.show\n",
    "function show(io::IO, value::Value)\n",
    "    print(io, \"Value(\",value.data,\")\")\n",
    "end\n",
    "\n",
    "\n",
    "import Base.==\n",
    "function ==(a::Value, b::Value)\n",
    "     return a===b\n",
    "end\n",
    "\n",
    "\n",
    "import Base.+\n",
    "function +(a::Value, b::Value)\n",
    "\n",
    "    out = a.data + b.data    \n",
    "    result = Value(out, 0.0, Operation(+, (a,b))) # Value(data, grad, op)\n",
    "    return result # this should be a Value\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "backprop!(val::Value{Nothing}) = nothing\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(+), ArgTypes}\n",
    "    \n",
    "    # val = a + b\n",
    "    # update a.grad, b.grad\n",
    "    \n",
    "    val.op.args[1].grad += val.grad\n",
    "    val.op.args[2].grad += val.grad\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function backward(a::Value)\n",
    "    \n",
    "    \n",
    "    function build_topo(v::Value, visited=Value[], topo=Value[])\n",
    "    \n",
    "        if !(v in visited)\n",
    "            \n",
    "            push!(visited, v)\n",
    "            \n",
    "            if v.op != nothing\n",
    "                for operand in v.op.args\n",
    "                    \n",
    "                    if operand isa Value\n",
    "                        build_topo(operand, visited, topo)\n",
    "                    end\n",
    "                end \n",
    "            end\n",
    "            \n",
    "            push!(topo, v) \n",
    "            \n",
    "            \n",
    "        end\n",
    "        return topo\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \n",
    "    topo = build_topo(a)\n",
    "    \n",
    "    a.grad = 1\n",
    "    #da/da = 1\n",
    "    \n",
    "    for node in reverse(topo)\n",
    "        backprop!(node)\n",
    "    end\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "Base.promote_rule(::Type{<:Value}, ::Type{T}) where {T<:Number} = Value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import Base.*\n",
    "function *(a::Value, b::Value)\n",
    "\n",
    "    out = a.data * b.data    \n",
    "    result = Value(out, 0.0, Operation(*, (a,b))) # Value(data, grad, op)\n",
    "    return result # this should be a Value\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(*), ArgTypes}\n",
    "    \n",
    "    # val = a * b\n",
    "    # update a.grad, b.grad\n",
    "    \n",
    "    val.op.args[1].grad += val.op.args[2].data * val.grad    \n",
    "    val.op.args[2].grad += val.op.args[1].data * val.grad\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "import Base.-\n",
    "\n",
    "# negation\n",
    "function -(a::Value)\n",
    "    \n",
    "    return a * -1\n",
    "    \n",
    "end\n",
    "\n",
    "# subtraction\n",
    "function -(a::Value, b::Value)\n",
    "    \n",
    "    return a + (-b)\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base.inv\n",
    "function inv(a::Value)\n",
    "    \n",
    "    out = 1.0 / a.data\n",
    "    result = Value(out, 0.0, Operation(inv, (a,))) # Value(data, grad, op)\n",
    "    return result # this should be a Value    \n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(inv), ArgTypes}\n",
    "    \n",
    "    # val = inv(a)\n",
    "    # update a.grad\n",
    "    \n",
    "    # a.grad -= (1.0 / a.data^2) * val.grad\n",
    "    \n",
    "    val.op.args[1].grad -= (1.0 / val.op.args[1].data^2) * val.grad\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base./\n",
    "function /(a::Value, b::Value)\n",
    "     \n",
    "    # a/b = a * b^(-1)\n",
    "    \n",
    "    return a * inv(b)\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base.tanh\n",
    "function tanh(a::Value)\n",
    "    \n",
    "    out = (exp(2 * a.data) - 1) / (exp(2 * a.data) + 1)\n",
    "    result = Value(out, 0.0, Operation(tanh, (a,))) # Value(data, grad, op)\n",
    "    return result # this should be a Value  \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(tanh), ArgTypes}\n",
    "\n",
    "    # val = tanh(a)\n",
    "    # update a.grad\n",
    "    \n",
    "    val.op.args[1].grad += (1 - val.data^2) * val.grad\n",
    "    \n",
    "\n",
    "end\n",
    "\n",
    "####################################################################################\n",
    "###### Tensors\n",
    "\n",
    "mutable struct Tensor{opType} <: AbstractArray{Float64, 2}\n",
    "    data::Array{Float64,2}\n",
    "    grad::Array{Float64,2}\n",
    "    op::opType\n",
    "end\n",
    "\n",
    "# 2D constructor -- Tensor(data, grad, op)\n",
    "Tensor(x::Array{Float64,2}) = Tensor(x, zeros(Float64,size(x)), nothing);\n",
    "\n",
    "# 1D constructor\n",
    "function Tensor(x::Array{Float64, 1}; column_vector::Bool=false)\n",
    "\n",
    "    if column_vector\n",
    "        # reshape x to column vector - size (N,1)\n",
    "        data_2D = reshape(x, (length(x),1))\n",
    "\n",
    "    else\n",
    "        # DEFAULT - row vector - size (1,N)\n",
    "        data_2D = reshape(x, (1, length(x)))\n",
    "\n",
    "    end\n",
    "\n",
    "    return Tensor(data_2D, zeros(Float64, size(data_2D)), nothing) # Tensor(data, grad, op)\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "import Base.show\n",
    "function show(io::IO, tensor::Tensor)\n",
    "    print(io, \"Tensor(\",tensor.data,\")\")\n",
    "end\n",
    "\n",
    "backprop!(tensor::Tensor{Nothing}) = nothing\n",
    "\n",
    "\n",
    "import Base.==\n",
    "function ==(a::Tensor, b::Tensor)\n",
    "     return a===b\n",
    "end\n",
    "\n",
    "\n",
    "Base.size(x::Tensor) = size(x.data)\n",
    "\n",
    "Base.getindex(x::Tensor, i...) = getindex(x.data, i...)\n",
    "\n",
    "Base.setindex!(x::Tensor, v, i...) = setindex!(x.data, v, i...)\n",
    "\n",
    "\n",
    "import Base.*\n",
    "function *(a::Tensor, b::Tensor)\n",
    "\n",
    "    out = a.data * b.data    \n",
    "    result = Tensor(out, zeros(Float64, size(out)), Operation(*, (a,b))) # Tensor(data, grad, op)\n",
    "    return result\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(*), ArgTypes}\n",
    "    \n",
    "    # tensor = a * b\n",
    "    # backprop!(tensor)\n",
    "    # update a.grad, b.grad\n",
    "    \n",
    "    tensor.op.args[1].grad += tensor.grad * transpose(tensor.op.args[2].data)\n",
    "    tensor.op.args[2].grad += transpose(tensor.op.args[1].data) * tensor.grad \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function backward(a::Tensor)\n",
    "    \n",
    "    \n",
    "    function build_topo(v::Tensor, visited=Tensor[], topo=Tensor[])\n",
    "    \n",
    "        if !(v in visited)\n",
    "            \n",
    "            push!(visited, v)\n",
    "            \n",
    "            if v.op != nothing\n",
    "                for operand in v.op.args\n",
    "                    \n",
    "                    if operand isa Tensor\n",
    "                        build_topo(operand, visited, topo)\n",
    "                    end\n",
    "                end \n",
    "            end\n",
    "            \n",
    "            push!(topo, v) \n",
    "            \n",
    "            \n",
    "        end\n",
    "        return topo\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \n",
    "    topo = build_topo(a)\n",
    "    \n",
    "    a.grad .= 1\n",
    "    #da/da = 1\n",
    "    \n",
    "    for node in reverse(topo)\n",
    "        backprop!(node)\n",
    "    end\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base.+\n",
    "function +(a::Tensor, b::Tensor)\n",
    "\n",
    "    # broadcasting happens automatically in case of row-vector\n",
    "    out = a.data .+ b.data    \n",
    "\n",
    "    result = Tensor(out, zeros(Float64, size(out)), Operation(+, (a,b))) # Tensor(data, grad, op)\n",
    "    return result\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(+), ArgTypes}\n",
    "    \n",
    "    # tensor = a + b\n",
    "    # backprop!(tensor)\n",
    "    # update a.grad, b.grad\n",
    "\n",
    "    if size(tensor.grad) == size(tensor.op.args[1].data)\n",
    "        tensor.op.args[1].grad += ones(size(tensor.op.args[1].data)) .* tensor.grad\n",
    "    else\n",
    "        # reverse broadcast\n",
    "        tensor.op.args[1].grad += ones(size(tensor.op.args[1].grad)) .* sum(tensor.grad,dims=1)\n",
    "    end\n",
    "\n",
    "    \n",
    "    if size(tensor.grad) == size(tensor.op.args[2].data)\n",
    "        tensor.op.args[2].grad += ones(size(tensor.op.args[2].data)) .* tensor.grad\n",
    "    else\n",
    "        # reverse broadcast\n",
    "        tensor.op.args[2].grad += ones(size(tensor.op.args[2].grad)) .* sum(tensor.grad,dims=1)\n",
    "    end\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function relu(a::Tensor)\n",
    "    \n",
    "    out = max.(0,a.data)\n",
    "    result = Tensor(out, zeros(Float64, size(out)), Operation(relu, (a,))) # Tensor(data, grad, op)\n",
    "    return result\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(relu), ArgTypes}\n",
    "\n",
    "    # tensor = relu(a)\n",
    "    # update a.grad\n",
    "    \n",
    "    tensor.op.args[1].grad += (tensor.op.args[1].data .> 0) .* tensor.grad\n",
    "    \n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d19273b-e09b-4410-b4e0-fdf8d75d1342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.6901885797569145; 1.007464451033138;;]\n",
      "[2.348826515395026]\n"
     ]
    }
   ],
   "source": [
    "inputs = Tensor(rand(2, 3)) # Matrix with shape (2,3) -- 2 samples, 3 input features per sample\n",
    "\n",
    "# first layer\n",
    "weights1 = Tensor(rand(3, 4)) # Matrix with shape (3,4) -- takes 3 inputs, has 4 neurons\n",
    "biases1 = Tensor([1.0, 1.0, 1.0, 1.0]) # Bias vector for first layer neurons\n",
    "\n",
    "# second layer\n",
    "weights2 = Tensor(rand( 4, 5)) # Matrix with shape (4,5) -- takes 4 inputs, has 5 neurons\n",
    "biases2 = Tensor([1.0, 1.0, 1.0, 1.0, 1.0]) # Bias vector for second layer neurons\n",
    "\n",
    "\n",
    "layer1_out = relu(inputs * weights1 + biases1)\n",
    "\n",
    "layer2_out = layer1_out * weights2 + biases2\n",
    "\n",
    "\n",
    "## softmax activation\n",
    "exp_values = exp.(layer2_out.data .- maximum(layer2_out.data, dims=2))\n",
    "probs = exp_values ./ sum(exp_values, dims=2)\n",
    "probs_clipped = clamp.(probs, 1e-7, 1-1e-7)\n",
    "\n",
    "\n",
    "## crossentropy loss, comparing final output to one-hot encoded true labels\n",
    "\n",
    "y_true = [0 1 0 0 0;\n",
    "          0 0 0 1 0]\n",
    "\n",
    "\n",
    "correct_confidences = sum(probs_clipped .* y_true, dims=2)\n",
    "\n",
    "sample_losses = -log.(correct_confidences)\n",
    "\n",
    "println(sample_losses)\n",
    "\n",
    "out = [sum(sample_losses) / length(sample_losses)]\n",
    "\n",
    "println(out)\n",
    "\n",
    "\n",
    "# println(layer2_out)\n",
    "# println(size(layer2_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e382a44-72bd-42b9-92a9-35dc09410ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function softmax_crossentropy(a::Tensor,y_true::Union{Array{Int,2},Array{Float64,2}}; grad::Bool=true)\n",
    "\n",
    "    # softmax activation\n",
    "    exp_values = exp.(a.data .- maximum(a.data, dims=2))\n",
    "    probs = exp_values ./ sum(exp_values, dims=2)\n",
    "\n",
    "    probs_clipped = clamp.(probs, 1e-7, 1 - 1e-7)\n",
    "    # deal with 0s and 1s\n",
    "\n",
    "    # basically just returns an array with the probability of the correct answer for each sample\n",
    "    correct_confidences = sum(probs_clipped .* y_true, dims=2)   \n",
    "\n",
    "    # negative log likelihood\n",
    "    sample_losses = -log.(correct_confidences)\n",
    "\n",
    "    # loss mean\n",
    "    out = [sum(sample_losses) / length(sample_losses)]\n",
    "\n",
    "\n",
    "    # ##\n",
    "    # if grad\n",
    "\n",
    "    #     # calculate and update a.grad\n",
    "\n",
    "    # end\n",
    "\n",
    "\n",
    "    # reshape out from (1,) to (1,1) \n",
    "    out = reshape(out, (1, 1))\n",
    "\n",
    "    result = Tensor(out, zeros(Float64, size(out)), Operation(softmax_crossentropy, (a,))) # Tensor(data, grad, op)\n",
    "\n",
    "    return result\n",
    "    \n",
    "\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
