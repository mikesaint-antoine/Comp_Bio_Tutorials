{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264ede42",
   "metadata": {},
   "source": [
    "# Neural Nets from Scratch in Julia\n",
    "\n",
    "## Lesson : Tanh\n",
    "\n",
    "* In this video we'll implement the tanh function for *Values*.\n",
    "* [Documentation site here](https://mikesaint-antoine.github.io/SimpleGrad.jl)\n",
    "* [Github repo here](https://github.com/mikesaint-antoine/SimpleGrad.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71397c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/ (generic function with 107 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## code so far\n",
    "\n",
    "mutable struct Value{opType} <: Number\n",
    "    data::Float64\n",
    "    grad::Float64\n",
    "    op::opType\n",
    "end\n",
    "\n",
    "\n",
    "struct Operation{FuncType, ArgTypes}\n",
    "    op::FuncType\n",
    "    args::ArgTypes\n",
    "end\n",
    "\n",
    "# constructor -- Value(data, grad, op)\n",
    "Value(x::Number) = Value(Float64(x), 0.0, nothing);\n",
    "\n",
    "\n",
    "import Base.show\n",
    "function show(io::IO, value::Value)\n",
    "    print(io, \"Value(\",value.data,\")\")\n",
    "end\n",
    "\n",
    "\n",
    "import Base.==\n",
    "function ==(a::Value, b::Value)\n",
    "     return a===b\n",
    "end\n",
    "\n",
    "\n",
    "import Base.+\n",
    "function +(a::Value, b::Value)\n",
    "\n",
    "    out = a.data + b.data    \n",
    "    result = Value(out, 0.0, Operation(+, (a,b))) # Value(data, grad, op)\n",
    "    return result # this should be a Value\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "backprop!(val::Value{Nothing}) = nothing\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(+), ArgTypes}\n",
    "    \n",
    "    # val = a + b\n",
    "    # update a.grad, b.grad\n",
    "    \n",
    "    val.op.args[1].grad += val.grad\n",
    "    val.op.args[2].grad += val.grad\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function backward(a::Value)\n",
    "    \n",
    "    \n",
    "    function build_topo(v::Value, visited=Value[], topo=Value[])\n",
    "    \n",
    "        if !(v in visited)\n",
    "            \n",
    "            push!(visited, v)\n",
    "            \n",
    "            if v.op != nothing\n",
    "                for operand in v.op.args\n",
    "                    \n",
    "                    if operand isa Value\n",
    "                        build_topo(operand, visited, topo)\n",
    "                    end\n",
    "                end \n",
    "            end\n",
    "            \n",
    "            push!(topo, v) \n",
    "            \n",
    "            \n",
    "        end\n",
    "        return topo\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \n",
    "    topo = build_topo(a)\n",
    "    \n",
    "    a.grad = 1\n",
    "    #da/da = 1\n",
    "    \n",
    "    for node in reverse(topo)\n",
    "        backprop!(node)\n",
    "    end\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "Base.promote_rule(::Type{<:Value}, ::Type{T}) where {T<:Number} = Value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import Base.*\n",
    "function *(a::Value, b::Value)\n",
    "\n",
    "    out = a.data * b.data    \n",
    "    result = Value(out, 0.0, Operation(*, (a,b))) # Value(data, grad, op)\n",
    "    return result # this should be a Value\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(*), ArgTypes}\n",
    "    \n",
    "    # val = a * b\n",
    "    # update a.grad, b.grad\n",
    "    \n",
    "    val.op.args[1].grad += val.op.args[2].data * val.grad    \n",
    "    val.op.args[2].grad += val.op.args[1].data * val.grad\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "import Base.-\n",
    "\n",
    "# negation\n",
    "function -(a::Value)\n",
    "    \n",
    "    return a * -1\n",
    "    \n",
    "end\n",
    "\n",
    "# subtraction\n",
    "function -(a::Value, b::Value)\n",
    "    \n",
    "    return a + (-b)\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base.inv\n",
    "function inv(a::Value)\n",
    "    \n",
    "    out = 1.0 / a.data\n",
    "    result = Value(out, 0.0, Operation(inv, (a,))) # Value(data, grad, op)\n",
    "    return result # this should be a Value    \n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(inv), ArgTypes}\n",
    "    \n",
    "    # val = inv(a)\n",
    "    # update a.grad\n",
    "    \n",
    "    # a.grad -= (1.0 / a.data^2) * val.grad\n",
    "    \n",
    "    val.op.args[1].grad -= (1.0 / val.op.args[1].data^2) * val.grad\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base./\n",
    "function /(a::Value, b::Value)\n",
    "     \n",
    "    # a/b = a * b^(-1)\n",
    "    \n",
    "    return a * inv(b)\n",
    "    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7d918b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop! (generic function with 5 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base.tanh\n",
    "function tanh(a::Value)\n",
    "    \n",
    "    out = (exp(2 * a.data) - 1) / (exp(2 * a.data) + 1)\n",
    "    result = Value(out, 0.0, Operation(tanh, (a,))) # Value(data, grad, op)\n",
    "    return result # this should be a Value  \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(tanh), ArgTypes}\n",
    "\n",
    "    # val = tanh(a)\n",
    "    # update a.grad\n",
    "    \n",
    "    val.op.args[1].grad += (1 - val.data^2) * val.grad\n",
    "    \n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tanh(input1*w1 + input2*w2 + input3*w3 + bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d73b6bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0259027224625243\n",
      "-3.082895447225581\n",
      "3.435226355479933\n",
      "0.8808272706358803\n"
     ]
    }
   ],
   "source": [
    "input1 = Value(2.3)\n",
    "input2 = Value(-3.5)\n",
    "input3 = Value(3.9)\n",
    "\n",
    "weight1 = Value(-0.8)\n",
    "weight2 = Value(1.8)\n",
    "weight3 = Value(3.0)\n",
    "\n",
    "bias = Value(-3.2)\n",
    "\n",
    "\n",
    "neuron_output = tanh(input1*weight1 + input2*weight2 + input3*weight3 + bias)\n",
    "\n",
    "\n",
    "backward(neuron_output)\n",
    "\n",
    "println(weight1.grad)\n",
    "println(weight2.grad)\n",
    "println(weight3.grad)\n",
    "\n",
    "println(bias.grad)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.7",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
