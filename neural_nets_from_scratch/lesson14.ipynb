{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc458033-47ba-45c8-99ea-06e96de739c7",
   "metadata": {},
   "source": [
    "# Neural Nets from Scratch in Julia\n",
    "\n",
    "## Lesson 14: Matrix addition for *Tensors*\n",
    "\n",
    "* In this video we'll write the forward and backward passes for *Tensor* matrix addition\n",
    "* [Documentation site here](https://mikesaint-antoine.github.io/SimpleGrad.jl)\n",
    "* [Github repo here](https://github.com/mikesaint-antoine/SimpleGrad.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614ab1a6-f2df-4418-b8c5-8775b0b8c7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## code so far\n",
    "\n",
    "\n",
    "struct Operation{FuncType, ArgTypes}\n",
    "    op::FuncType\n",
    "    args::ArgTypes\n",
    "end\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "###### Values\n",
    "\n",
    "mutable struct Value{opType} <: Number\n",
    "    data::Float64\n",
    "    grad::Float64\n",
    "    op::opType\n",
    "end\n",
    "\n",
    "# constructor -- Value(data, grad, op)\n",
    "Value(x::Number) = Value(Float64(x), 0.0, nothing);\n",
    "\n",
    "\n",
    "import Base.show\n",
    "function show(io::IO, value::Value)\n",
    "    print(io, \"Value(\",value.data,\")\")\n",
    "end\n",
    "\n",
    "\n",
    "import Base.==\n",
    "function ==(a::Value, b::Value)\n",
    "     return a===b\n",
    "end\n",
    "\n",
    "\n",
    "import Base.+\n",
    "function +(a::Value, b::Value)\n",
    "\n",
    "    out = a.data + b.data    \n",
    "    result = Value(out, 0.0, Operation(+, (a,b))) # Value(data, grad, op)\n",
    "    return result # this should be a Value\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "backprop!(val::Value{Nothing}) = nothing\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(+), ArgTypes}\n",
    "    \n",
    "    # val = a + b\n",
    "    # update a.grad, b.grad\n",
    "    \n",
    "    val.op.args[1].grad += val.grad\n",
    "    val.op.args[2].grad += val.grad\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function backward(a::Value)\n",
    "    \n",
    "    \n",
    "    function build_topo(v::Value, visited=Value[], topo=Value[])\n",
    "    \n",
    "        if !(v in visited)\n",
    "            \n",
    "            push!(visited, v)\n",
    "            \n",
    "            if v.op != nothing\n",
    "                for operand in v.op.args\n",
    "                    \n",
    "                    if operand isa Value\n",
    "                        build_topo(operand, visited, topo)\n",
    "                    end\n",
    "                end \n",
    "            end\n",
    "            \n",
    "            push!(topo, v) \n",
    "            \n",
    "            \n",
    "        end\n",
    "        return topo\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \n",
    "    topo = build_topo(a)\n",
    "    \n",
    "    a.grad = 1\n",
    "    #da/da = 1\n",
    "    \n",
    "    for node in reverse(topo)\n",
    "        backprop!(node)\n",
    "    end\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "Base.promote_rule(::Type{<:Value}, ::Type{T}) where {T<:Number} = Value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import Base.*\n",
    "function *(a::Value, b::Value)\n",
    "\n",
    "    out = a.data * b.data    \n",
    "    result = Value(out, 0.0, Operation(*, (a,b))) # Value(data, grad, op)\n",
    "    return result # this should be a Value\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(*), ArgTypes}\n",
    "    \n",
    "    # val = a * b\n",
    "    # update a.grad, b.grad\n",
    "    \n",
    "    val.op.args[1].grad += val.op.args[2].data * val.grad    \n",
    "    val.op.args[2].grad += val.op.args[1].data * val.grad\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "import Base.-\n",
    "\n",
    "# negation\n",
    "function -(a::Value)\n",
    "    \n",
    "    return a * -1\n",
    "    \n",
    "end\n",
    "\n",
    "# subtraction\n",
    "function -(a::Value, b::Value)\n",
    "    \n",
    "    return a + (-b)\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base.inv\n",
    "function inv(a::Value)\n",
    "    \n",
    "    out = 1.0 / a.data\n",
    "    result = Value(out, 0.0, Operation(inv, (a,))) # Value(data, grad, op)\n",
    "    return result # this should be a Value    \n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(inv), ArgTypes}\n",
    "    \n",
    "    # val = inv(a)\n",
    "    # update a.grad\n",
    "    \n",
    "    # a.grad -= (1.0 / a.data^2) * val.grad\n",
    "    \n",
    "    val.op.args[1].grad -= (1.0 / val.op.args[1].data^2) * val.grad\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base./\n",
    "function /(a::Value, b::Value)\n",
    "     \n",
    "    # a/b = a * b^(-1)\n",
    "    \n",
    "    return a * inv(b)\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base.tanh\n",
    "function tanh(a::Value)\n",
    "    \n",
    "    out = (exp(2 * a.data) - 1) / (exp(2 * a.data) + 1)\n",
    "    result = Value(out, 0.0, Operation(tanh, (a,))) # Value(data, grad, op)\n",
    "    return result # this should be a Value  \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(tanh), ArgTypes}\n",
    "\n",
    "    # val = tanh(a)\n",
    "    # update a.grad\n",
    "    \n",
    "    val.op.args[1].grad += (1 - val.data^2) * val.grad\n",
    "    \n",
    "\n",
    "end\n",
    "\n",
    "####################################################################################\n",
    "###### Tensors\n",
    "\n",
    "mutable struct Tensor{opType} <: AbstractArray{Float64, 2}\n",
    "    data::Array{Float64,2}\n",
    "    grad::Array{Float64,2}\n",
    "    op::opType\n",
    "end\n",
    "\n",
    "# 2D constructor -- Tensor(data, grad, op)\n",
    "Tensor(x::Array{Float64,2}) = Tensor(x, zeros(Float64,size(x)), nothing);\n",
    "\n",
    "# 1D constructor\n",
    "function Tensor(x::Array{Float64, 1}; column_vector::Bool=false)\n",
    "\n",
    "    if column_vector\n",
    "        # reshape x to column vector - size (N,1)\n",
    "        data_2D = reshape(x, (length(x),1))\n",
    "\n",
    "    else\n",
    "        # DEFAULT - row vector - size (1,N)\n",
    "        data_2D = reshape(x, (1, length(x)))\n",
    "\n",
    "    end\n",
    "\n",
    "    return Tensor(data_2D, zeros(Float64, size(data_2D)), nothing) # Tensor(data, grad, op)\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "import Base.show\n",
    "function show(io::IO, tensor::Tensor)\n",
    "    print(io, \"Tensor(\",tensor.data,\")\")\n",
    "end\n",
    "\n",
    "backprop!(tensor::Tensor{Nothing}) = nothing\n",
    "\n",
    "\n",
    "import Base.==\n",
    "function ==(a::Tensor, b::Tensor)\n",
    "     return a===b\n",
    "end\n",
    "\n",
    "\n",
    "Base.size(x::Tensor) = size(x.data)\n",
    "\n",
    "Base.getindex(x::Tensor, i...) = getindex(x.data, i...)\n",
    "\n",
    "Base.setindex!(x::Tensor, v, i...) = setindex!(x.data, v, i...)\n",
    "\n",
    "\n",
    "import Base.*\n",
    "function *(a::Tensor, b::Tensor)\n",
    "\n",
    "    out = a.data * b.data    \n",
    "    result = Tensor(out, zeros(Float64, size(out)), Operation(*, (a,b))) # Tensor(data, grad, op)\n",
    "    return result\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(*), ArgTypes}\n",
    "    \n",
    "    # tensor = a * b\n",
    "    # backprop!(tensor)\n",
    "    # update a.grad, b.grad\n",
    "    \n",
    "    tensor.op.args[1].grad += tensor.grad * transpose(tensor.op.args[2].data)\n",
    "    tensor.op.args[2].grad += transpose(tensor.op.args[1].data) * tensor.grad \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function backward(a::Tensor)\n",
    "    \n",
    "    \n",
    "    function build_topo(v::Tensor, visited=Tensor[], topo=Tensor[])\n",
    "    \n",
    "        if !(v in visited)\n",
    "            \n",
    "            push!(visited, v)\n",
    "            \n",
    "            if v.op != nothing\n",
    "                for operand in v.op.args\n",
    "                    \n",
    "                    if operand isa Tensor\n",
    "                        build_topo(operand, visited, topo)\n",
    "                    end\n",
    "                end \n",
    "            end\n",
    "            \n",
    "            push!(topo, v) \n",
    "            \n",
    "            \n",
    "        end\n",
    "        return topo\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \n",
    "    topo = build_topo(a)\n",
    "    \n",
    "    a.grad .= 1\n",
    "    #da/da = 1\n",
    "    \n",
    "    for node in reverse(topo)\n",
    "        backprop!(node)\n",
    "    end\n",
    "    \n",
    "    \n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af215dbc-ea9f-4f19-94c6-728077cf5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple case - a and b are same shape\n",
    "\n",
    "# a = [1.0 2.0 3.0; 4.0 5.0 6.0]\n",
    "\n",
    "# b = [1.0 1.0 1.0; 2.0 2.0 2.0]\n",
    "\n",
    "# c = a .+ b\n",
    "\n",
    "# println(c)\n",
    "\n",
    "# broadcasting\n",
    "\n",
    "# a = [1.0 2.0 3.0; 4.0 5.0 6.0]\n",
    "\n",
    "# b = [1.0 1.0 1.0]\n",
    "\n",
    "# c = a .+ b\n",
    "\n",
    "# println(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "043825fe-80f9-4f50-af8a-60b97f4f71bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+ (generic function with 191 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base.+\n",
    "function +(a::Tensor, b::Tensor)\n",
    "\n",
    "    # broadcasting happens automatically in case of row-vector\n",
    "    out = a.data .+ b.data    \n",
    "\n",
    "    result = Tensor(out, zeros(Float64, size(out)), Operation(+, (a,b))) # Tensor(data, grad, op)\n",
    "    return result\n",
    " \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "916c7712-e5e4-4ad2-91b6-765024a4d017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop! (generic function with 8 methods)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(+), ArgTypes}\n",
    "    \n",
    "    # tensor = a + b\n",
    "    # backprop!(tensor)\n",
    "    # update a.grad, b.grad\n",
    "\n",
    "    if size(tensor.grad) == size(tensor.op.args[1].data)\n",
    "        tensor.op.args[1].grad += ones(size(tensor.op.args[1].data)) .* tensor.grad\n",
    "    else\n",
    "        # reverse broadcast\n",
    "        tensor.op.args[1].grad += ones(size(tensor.op.args[1].grad)) .* sum(tensor.grad,dims=1)\n",
    "    end\n",
    "\n",
    "    \n",
    "    if size(tensor.grad) == size(tensor.op.args[2].data)\n",
    "        tensor.op.args[2].grad += ones(size(tensor.op.args[2].data)) .* tensor.grad\n",
    "    else\n",
    "        # reverse broadcast\n",
    "        tensor.op.args[2].grad += ones(size(tensor.op.args[2].grad)) .* sum(tensor.grad,dims=1)\n",
    "    end\n",
    "    \n",
    "    \n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
