{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b916c4-eed3-4c44-a487-101331ac6b52",
   "metadata": {},
   "source": [
    "# Neural Nets from Scratch in Julia\n",
    "\n",
    "## Lesson 18: Solving MNIST\n",
    "\n",
    "* In this video we'll finally put everything together make a neural network model that can classify handwritten digits, using the MNIST dataset. The dataset used in this video can be downloaded [here.](https://www.kaggle.com/datasets/scolianni/mnistasjpg?resource=download)\n",
    "* [Documentation site here](https://mikesaint-antoine.github.io/SimpleGrad.jl)\n",
    "* [Github repo here](https://github.com/mikesaint-antoine/SimpleGrad.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a4275c-e6d5-40f6-a66e-4f3b554db2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop! (generic function with 10 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## code so far\n",
    "\n",
    "\n",
    "struct Operation{FuncType, ArgTypes}\n",
    "    op::FuncType\n",
    "    args::ArgTypes\n",
    "end\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "###### Values\n",
    "\n",
    "mutable struct Value{opType} <: Number\n",
    "    data::Float64\n",
    "    grad::Float64\n",
    "    op::opType\n",
    "end\n",
    "\n",
    "# constructor -- Value(data, grad, op)\n",
    "Value(x::Number) = Value(Float64(x), 0.0, nothing);\n",
    "\n",
    "\n",
    "import Base.show\n",
    "function show(io::IO, value::Value)\n",
    "    print(io, \"Value(\",value.data,\")\")\n",
    "end\n",
    "\n",
    "\n",
    "import Base.==\n",
    "function ==(a::Value, b::Value)\n",
    "     return a===b\n",
    "end\n",
    "\n",
    "\n",
    "import Base.+\n",
    "function +(a::Value, b::Value)\n",
    "\n",
    "    out = a.data + b.data    \n",
    "    result = Value(out, 0.0, Operation(+, (a,b))) # Value(data, grad, op)\n",
    "    return result # this should be a Value\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "backprop!(val::Value{Nothing}) = nothing\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(+), ArgTypes}\n",
    "    \n",
    "    # val = a + b\n",
    "    # update a.grad, b.grad\n",
    "    \n",
    "    val.op.args[1].grad += val.grad\n",
    "    val.op.args[2].grad += val.grad\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function backward(a::Value)\n",
    "    \n",
    "    \n",
    "    function build_topo(v::Value, visited=Value[], topo=Value[])\n",
    "    \n",
    "        if !(v in visited)\n",
    "            \n",
    "            push!(visited, v)\n",
    "            \n",
    "            if v.op != nothing\n",
    "                for operand in v.op.args\n",
    "                    \n",
    "                    if operand isa Value\n",
    "                        build_topo(operand, visited, topo)\n",
    "                    end\n",
    "                end \n",
    "            end\n",
    "            \n",
    "            push!(topo, v) \n",
    "            \n",
    "            \n",
    "        end\n",
    "        return topo\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \n",
    "    topo = build_topo(a)\n",
    "    \n",
    "    a.grad = 1\n",
    "    #da/da = 1\n",
    "    \n",
    "    for node in reverse(topo)\n",
    "        backprop!(node)\n",
    "    end\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "Base.promote_rule(::Type{<:Value}, ::Type{T}) where {T<:Number} = Value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import Base.*\n",
    "function *(a::Value, b::Value)\n",
    "\n",
    "    out = a.data * b.data    \n",
    "    result = Value(out, 0.0, Operation(*, (a,b))) # Value(data, grad, op)\n",
    "    return result # this should be a Value\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(*), ArgTypes}\n",
    "    \n",
    "    # val = a * b\n",
    "    # update a.grad, b.grad\n",
    "    \n",
    "    val.op.args[1].grad += val.op.args[2].data * val.grad    \n",
    "    val.op.args[2].grad += val.op.args[1].data * val.grad\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "import Base.-\n",
    "\n",
    "# negation\n",
    "function -(a::Value)\n",
    "    \n",
    "    return a * -1\n",
    "    \n",
    "end\n",
    "\n",
    "# subtraction\n",
    "function -(a::Value, b::Value)\n",
    "    \n",
    "    return a + (-b)\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base.inv\n",
    "function inv(a::Value)\n",
    "    \n",
    "    out = 1.0 / a.data\n",
    "    result = Value(out, 0.0, Operation(inv, (a,))) # Value(data, grad, op)\n",
    "    return result # this should be a Value    \n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(inv), ArgTypes}\n",
    "    \n",
    "    # val = inv(a)\n",
    "    # update a.grad\n",
    "    \n",
    "    # a.grad -= (1.0 / a.data^2) * val.grad\n",
    "    \n",
    "    val.op.args[1].grad -= (1.0 / val.op.args[1].data^2) * val.grad\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base./\n",
    "function /(a::Value, b::Value)\n",
    "     \n",
    "    # a/b = a * b^(-1)\n",
    "    \n",
    "    return a * inv(b)\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base.tanh\n",
    "function tanh(a::Value)\n",
    "    \n",
    "    out = (exp(2 * a.data) - 1) / (exp(2 * a.data) + 1)\n",
    "    result = Value(out, 0.0, Operation(tanh, (a,))) # Value(data, grad, op)\n",
    "    return result # this should be a Value  \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(tanh), ArgTypes}\n",
    "\n",
    "    # val = tanh(a)\n",
    "    # update a.grad\n",
    "    \n",
    "    val.op.args[1].grad += (1 - val.data^2) * val.grad\n",
    "    \n",
    "\n",
    "end\n",
    "\n",
    "####################################################################################\n",
    "###### Tensors\n",
    "\n",
    "mutable struct Tensor{opType} <: AbstractArray{Float64, 2}\n",
    "    data::Array{Float64,2}\n",
    "    grad::Array{Float64,2}\n",
    "    op::opType\n",
    "end\n",
    "\n",
    "# 2D constructor -- Tensor(data, grad, op)\n",
    "Tensor(x::Array{Float64,2}) = Tensor(x, zeros(Float64,size(x)), nothing);\n",
    "\n",
    "# 1D constructor\n",
    "function Tensor(x::Array{Float64, 1}; column_vector::Bool=false)\n",
    "\n",
    "    if column_vector\n",
    "        # reshape x to column vector - size (N,1)\n",
    "        data_2D = reshape(x, (length(x),1))\n",
    "\n",
    "    else\n",
    "        # DEFAULT - row vector - size (1,N)\n",
    "        data_2D = reshape(x, (1, length(x)))\n",
    "\n",
    "    end\n",
    "\n",
    "    return Tensor(data_2D, zeros(Float64, size(data_2D)), nothing) # Tensor(data, grad, op)\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "import Base.show\n",
    "function show(io::IO, tensor::Tensor)\n",
    "    print(io, \"Tensor(\",tensor.data,\")\")\n",
    "end\n",
    "\n",
    "backprop!(tensor::Tensor{Nothing}) = nothing\n",
    "\n",
    "\n",
    "import Base.==\n",
    "function ==(a::Tensor, b::Tensor)\n",
    "     return a===b\n",
    "end\n",
    "\n",
    "\n",
    "Base.size(x::Tensor) = size(x.data)\n",
    "\n",
    "Base.getindex(x::Tensor, i...) = getindex(x.data, i...)\n",
    "\n",
    "Base.setindex!(x::Tensor, v, i...) = setindex!(x.data, v, i...)\n",
    "\n",
    "\n",
    "import Base.*\n",
    "function *(a::Tensor, b::Tensor)\n",
    "\n",
    "    out = a.data * b.data    \n",
    "    result = Tensor(out, zeros(Float64, size(out)), Operation(*, (a,b))) # Tensor(data, grad, op)\n",
    "    return result\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(*), ArgTypes}\n",
    "    \n",
    "    # tensor = a * b\n",
    "    # backprop!(tensor)\n",
    "    # update a.grad, b.grad\n",
    "    \n",
    "    tensor.op.args[1].grad += tensor.grad * transpose(tensor.op.args[2].data)\n",
    "    tensor.op.args[2].grad += transpose(tensor.op.args[1].data) * tensor.grad \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function backward(a::Tensor)\n",
    "    \n",
    "    \n",
    "    function build_topo(v::Tensor, visited=Tensor[], topo=Tensor[])\n",
    "    \n",
    "        if !(v in visited)\n",
    "            \n",
    "            push!(visited, v)\n",
    "            \n",
    "            if v.op != nothing\n",
    "                for operand in v.op.args\n",
    "                    \n",
    "                    if operand isa Tensor\n",
    "                        build_topo(operand, visited, topo)\n",
    "                    end\n",
    "                end \n",
    "            end\n",
    "            \n",
    "            push!(topo, v) \n",
    "            \n",
    "            \n",
    "        end\n",
    "        return topo\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \n",
    "    topo = build_topo(a)\n",
    "    \n",
    "    a.grad .= 1\n",
    "    #da/da = 1\n",
    "    \n",
    "    for node in reverse(topo)\n",
    "        backprop!(node)\n",
    "    end\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base.+\n",
    "function +(a::Tensor, b::Tensor)\n",
    "\n",
    "    # broadcasting happens automatically in case of row-vector\n",
    "    out = a.data .+ b.data    \n",
    "\n",
    "    result = Tensor(out, zeros(Float64, size(out)), Operation(+, (a,b))) # Tensor(data, grad, op)\n",
    "    return result\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(+), ArgTypes}\n",
    "    \n",
    "    # tensor = a + b\n",
    "    # backprop!(tensor)\n",
    "    # update a.grad, b.grad\n",
    "\n",
    "    if size(tensor.grad) == size(tensor.op.args[1].data)\n",
    "        tensor.op.args[1].grad += ones(size(tensor.op.args[1].data)) .* tensor.grad\n",
    "    else\n",
    "        # reverse broadcast\n",
    "        tensor.op.args[1].grad += ones(size(tensor.op.args[1].grad)) .* sum(tensor.grad,dims=1)\n",
    "    end\n",
    "\n",
    "    \n",
    "    if size(tensor.grad) == size(tensor.op.args[2].data)\n",
    "        tensor.op.args[2].grad += ones(size(tensor.op.args[2].data)) .* tensor.grad\n",
    "    else\n",
    "        # reverse broadcast\n",
    "        tensor.op.args[2].grad += ones(size(tensor.op.args[2].grad)) .* sum(tensor.grad,dims=1)\n",
    "    end\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function relu(a::Tensor)\n",
    "    \n",
    "    out = max.(0,a.data)\n",
    "    result = Tensor(out, zeros(Float64, size(out)), Operation(relu, (a,))) # Tensor(data, grad, op)\n",
    "    return result\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(relu), ArgTypes}\n",
    "\n",
    "    # tensor = relu(a)\n",
    "    # update a.grad\n",
    "    \n",
    "    tensor.op.args[1].grad += (tensor.op.args[1].data .> 0) .* tensor.grad\n",
    "    \n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "function softmax_crossentropy(a::Tensor,y_true::Union{Array{Int,2},Array{Float64,2}}; grad::Bool=true)\n",
    "\n",
    "    # softmax activation\n",
    "    exp_values = exp.(a.data .- maximum(a.data, dims=2))\n",
    "    probs = exp_values ./ sum(exp_values, dims=2)\n",
    "\n",
    "    probs_clipped = clamp.(probs, 1e-7, 1 - 1e-7)\n",
    "    # deal with 0s and 1s\n",
    "\n",
    "    # basically just returns an array with the probability of the correct answer for each sample\n",
    "    correct_confidences = sum(probs_clipped .* y_true, dims=2)   \n",
    "\n",
    "    # negative log likelihood\n",
    "    sample_losses = -log.(correct_confidences)\n",
    "\n",
    "    # loss mean\n",
    "    out = [sum(sample_losses) / length(sample_losses)]\n",
    "\n",
    "\n",
    "    # ##\n",
    "    if grad\n",
    "\n",
    "        # calculate and update a.grad\n",
    "\n",
    "        samples = size(probs,1)\n",
    "        \n",
    "        \n",
    "        y_true_argmax = argmax(y_true, dims=2)\n",
    "        \n",
    "        a.grad = copy(probs)\n",
    "        \n",
    "        \n",
    "        for samp_ind in 1:samples\n",
    "            a.grad[samp_ind, y_true_argmax[samp_ind][2]] -=1\n",
    "        end\n",
    "        \n",
    "        a.grad = a.grad ./ samples\n",
    "\n",
    "    end\n",
    "\n",
    "\n",
    "    # reshape out from (1,) to (1,1) \n",
    "    out = reshape(out, (1, 1))\n",
    "\n",
    "    result = Tensor(out, zeros(Float64, size(out)), Operation(softmax_crossentropy, (a,))) # Tensor(data, grad, op)\n",
    "\n",
    "    return result\n",
    "    \n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "# softmax_crossentropy backprop is empty because gradient is easier to calculate during forward pass\n",
    "function backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(softmax_crossentropy), ArgTypes}\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "497e23df-b7ef-49ff-a39d-e4e2673fa989",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data link: https://www.kaggle.com/datasets/scolianni/mnistasjpg\n",
    "\n",
    "# Pkg.add(\"Images\")\n",
    "using Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73593bf0-4747-4927-9eda-ac55d3a53946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "# basic image processing\n",
    "\n",
    "img_path = \"archive/trainingSet/trainingSet/0/img_1.jpg\"\n",
    "\n",
    "img = load(img_path)\n",
    "\n",
    "img_mat = Float64.(img)\n",
    "\n",
    "img_flattened = reshape(img_mat, :)\n",
    "\n",
    "# println(img_mat)\n",
    "println(size(img_flattened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c041c261-7ded-4ebf-9015-27136a2b6507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "(42000,)\n"
     ]
    }
   ],
   "source": [
    "# reading in images\n",
    "\n",
    "base_path = \"archive/trainingSet/trainingSet/\"\n",
    "\n",
    "X = [] # image pixel data. (N,784)\n",
    "y = [] # digit label. (N,)\n",
    "\n",
    "\n",
    "for digit in 0:9\n",
    "    folder_path = joinpath(base_path, string(digit))\n",
    "    for file in readdir(folder_path)\n",
    "        img_path = joinpath(folder_path, file)\n",
    "        img = load(img_path)\n",
    "        img_mat = Float64.(img)\n",
    "        img_flattened = reshape(img_mat, :)\n",
    "        push!(X, img_flattened)\n",
    "        push!(y, digit)  \n",
    "        \n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "X = hcat(X...)' # transposing to (N, 784)\n",
    "\n",
    "println(size(X))\n",
    "println(size(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1030893b-3bbe-43be-aad2-160aebdfe3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "n = size(X,1)\n",
    "\n",
    "perm = shuffle(1:n)\n",
    "X = X[perm, :]\n",
    "y = y[perm];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd0c88a1-239f-4313-a607-cc37a34b1f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33600, 784)\n",
      "(33600,)\n",
      "(8400, 784)\n",
      "(8400,)\n"
     ]
    }
   ],
   "source": [
    "train_size = Int(0.8 * size(X,1))\n",
    "\n",
    "X_train = X[1:train_size, :]\n",
    "y_train = y[1:train_size]\n",
    "\n",
    "X_test = X[train_size+1:end, :]\n",
    "y_test = y[train_size+1:end]\n",
    "\n",
    "println(size(X_train))\n",
    "println(size(y_train))\n",
    "\n",
    "println(size(X_test))\n",
    "println(size(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8004df1-2a1e-4d9d-be13-1ec72bebe4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing parameters\n",
    "weights1 = Tensor(0.01 * rand(784, 128)) # taking in 784 input features, has 128 neurons in the layer\n",
    "biases1 = Tensor(zeros(128))\n",
    "weights2 = Tensor(0.01 * rand(128, 10)) # taking 128 inputs (from 128 neurons in the first layer), has 10 neurons in the layer\n",
    "biases2 = Tensor(zeros(10))\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "lr = 0.1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40ce3f07-7daa-4718-9cdf-288fe87905fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, run: 10, loss: 2.302\n",
      "Epoch: 1, run: 20, loss: 2.271\n",
      "Epoch: 1, run: 30, loss: 2.267\n",
      "Epoch: 1, run: 40, loss: 2.255\n",
      "Epoch: 1, run: 50, loss: 2.152\n",
      "Epoch: 1, run: 60, loss: 2.06\n",
      "Epoch: 1, run: 70, loss: 1.898\n",
      "Epoch: 1, run: 80, loss: 1.636\n",
      "Epoch: 1, run: 90, loss: 1.567\n",
      "Epoch: 1, run: 100, loss: 1.304\n",
      "Epoch: 1, run: 110, loss: 1.077\n",
      "Epoch: 1, run: 120, loss: 1.019\n",
      "Epoch: 1, run: 130, loss: 0.849\n",
      "Epoch: 1, run: 140, loss: 0.842\n",
      "Epoch: 1, run: 150, loss: 0.866\n",
      "Epoch: 1, run: 160, loss: 0.751\n",
      "Epoch: 1, run: 170, loss: 0.672\n",
      "Epoch: 1, run: 180, loss: 0.711\n",
      "Epoch: 1, run: 190, loss: 0.799\n",
      "Epoch: 1, run: 200, loss: 0.507\n",
      "Epoch: 1, run: 210, loss: 0.549\n",
      "Epoch: 1, run: 220, loss: 0.504\n",
      "Epoch: 1, run: 230, loss: 0.511\n",
      "Epoch: 1, run: 240, loss: 0.418\n",
      "Epoch: 1, run: 250, loss: 0.556\n",
      "Epoch: 1, run: 260, loss: 0.501\n",
      "Epoch: 1, run: 270, loss: 0.567\n",
      "Epoch: 1, run: 280, loss: 0.479\n",
      "Epoch: 1, run: 290, loss: 0.467\n",
      "Epoch: 1, run: 300, loss: 0.629\n",
      "Epoch: 1, run: 310, loss: 0.481\n",
      "Epoch: 1, run: 320, loss: 0.402\n",
      "Epoch: 1, run: 330, loss: 0.439\n",
      "Epoch: 2, run: 340, loss: 0.338\n",
      "Epoch: 2, run: 350, loss: 0.451\n",
      "Epoch: 2, run: 360, loss: 0.317\n",
      "Epoch: 2, run: 370, loss: 0.545\n",
      "Epoch: 2, run: 380, loss: 0.309\n",
      "Epoch: 2, run: 390, loss: 0.386\n",
      "Epoch: 2, run: 400, loss: 0.387\n",
      "Epoch: 2, run: 410, loss: 0.355\n",
      "Epoch: 2, run: 420, loss: 0.474\n",
      "Epoch: 2, run: 430, loss: 0.251\n",
      "Epoch: 2, run: 440, loss: 0.405\n",
      "Epoch: 2, run: 450, loss: 0.339\n",
      "Epoch: 2, run: 460, loss: 0.328\n",
      "Epoch: 2, run: 470, loss: 0.405\n",
      "Epoch: 2, run: 480, loss: 0.446\n",
      "Epoch: 2, run: 490, loss: 0.256\n",
      "Epoch: 2, run: 500, loss: 0.391\n",
      "Epoch: 2, run: 510, loss: 0.287\n",
      "Epoch: 2, run: 520, loss: 0.274\n",
      "Epoch: 2, run: 530, loss: 0.314\n",
      "Epoch: 2, run: 540, loss: 0.35\n",
      "Epoch: 2, run: 550, loss: 0.218\n",
      "Epoch: 2, run: 560, loss: 0.412\n",
      "Epoch: 2, run: 570, loss: 0.266\n",
      "Epoch: 2, run: 580, loss: 0.411\n",
      "Epoch: 2, run: 590, loss: 0.311\n",
      "Epoch: 2, run: 600, loss: 0.331\n",
      "Epoch: 2, run: 610, loss: 0.346\n",
      "Epoch: 2, run: 620, loss: 0.176\n",
      "Epoch: 2, run: 630, loss: 0.302\n",
      "Epoch: 2, run: 640, loss: 0.281\n",
      "Epoch: 2, run: 650, loss: 0.445\n",
      "Epoch: 2, run: 660, loss: 0.416\n",
      "Epoch: 2, run: 670, loss: 0.427\n",
      "Epoch: 3, run: 680, loss: 0.311\n",
      "Epoch: 3, run: 690, loss: 0.385\n",
      "Epoch: 3, run: 700, loss: 0.408\n",
      "Epoch: 3, run: 710, loss: 0.162\n",
      "Epoch: 3, run: 720, loss: 0.137\n",
      "Epoch: 3, run: 730, loss: 0.367\n",
      "Epoch: 3, run: 740, loss: 0.346\n",
      "Epoch: 3, run: 750, loss: 0.243\n",
      "Epoch: 3, run: 760, loss: 0.482\n",
      "Epoch: 3, run: 770, loss: 0.197\n",
      "Epoch: 3, run: 780, loss: 0.564\n",
      "Epoch: 3, run: 790, loss: 0.353\n",
      "Epoch: 3, run: 800, loss: 0.289\n",
      "Epoch: 3, run: 810, loss: 0.399\n",
      "Epoch: 3, run: 820, loss: 0.423\n",
      "Epoch: 3, run: 830, loss: 0.287\n",
      "Epoch: 3, run: 840, loss: 0.238\n",
      "Epoch: 3, run: 850, loss: 0.328\n",
      "Epoch: 3, run: 860, loss: 0.174\n",
      "Epoch: 3, run: 870, loss: 0.268\n",
      "Epoch: 3, run: 880, loss: 0.215\n",
      "Epoch: 3, run: 890, loss: 0.245\n",
      "Epoch: 3, run: 900, loss: 0.347\n",
      "Epoch: 3, run: 910, loss: 0.229\n",
      "Epoch: 3, run: 920, loss: 0.183\n",
      "Epoch: 3, run: 930, loss: 0.386\n",
      "Epoch: 3, run: 940, loss: 0.368\n",
      "Epoch: 3, run: 950, loss: 0.282\n",
      "Epoch: 3, run: 960, loss: 0.46\n",
      "Epoch: 3, run: 970, loss: 0.185\n",
      "Epoch: 3, run: 980, loss: 0.239\n",
      "Epoch: 3, run: 990, loss: 0.203\n",
      "Epoch: 3, run: 1000, loss: 0.349\n"
     ]
    }
   ],
   "source": [
    "\n",
    "global run = 1\n",
    "for epoch in 1:epochs\n",
    "    for i in 1:batch_size:size(X_train,1)\n",
    "    \n",
    "    \n",
    "        # size of input matrix = (batch_size, 784)\n",
    "        batch_X = Tensor(X_train[i:i+batch_size-1, :])\n",
    "        batch_y = y_train[i:i+batch_size-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## convert batch_y to one-hot\n",
    "        batch_y_one_hot = zeros(batch_size,num_classes)\n",
    "        for batch_ind in 1:batch_size\n",
    "            batch_y_one_hot[batch_ind,Int.(batch_y)[batch_ind]+1] = 1\n",
    "        end\n",
    "        \n",
    "        \n",
    "        ## zero grads\n",
    "        weights1.grad .= 0\n",
    "        weights2.grad .= 0\n",
    "        biases1.grad .= 0\n",
    "        biases2.grad .= 0\n",
    "        \n",
    "        \n",
    "        # layer 1\n",
    "        layer1_out = relu(batch_X * weights1 + biases1);\n",
    "        \n",
    "        # layer 2\n",
    "        layer2_out = layer1_out * weights2 + biases2\n",
    "        \n",
    "        \n",
    "        loss = softmax_crossentropy(layer2_out,batch_y_one_hot)\n",
    "            \n",
    "        \n",
    "        backward(loss)\n",
    "        \n",
    "        \n",
    "        # updating parameter values based on gradient\n",
    "        weights1.data = weights1.data - weights1.grad .* lr\n",
    "        biases1.data = biases1.data - biases1.grad .* lr\n",
    "        weights2.data = weights2.data - weights2.grad .* lr\n",
    "        biases2.data = biases2.data - biases2.grad .* lr;\n",
    "\n",
    "\n",
    "        if run % 10 == 0\n",
    "            println(\"Epoch: $epoch, run: $run, loss: $(round(loss.data[1], digits=3))\")\n",
    "        end\n",
    "        \n",
    "        global run += 1\n",
    "    \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2eceab9-7875-4903-9e9b-ed19fbd461f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:\n",
      "0.9188095238095239\n"
     ]
    }
   ],
   "source": [
    "global correct = 0\n",
    "global total = 0\n",
    "for i in 1:length(y_test)\n",
    "    X_in = X_test[i:i,:] ## need to keep this (1,784), not (784,)\n",
    "    X_in = Tensor(X_in)\n",
    "    y_true = y_test[i]\n",
    "\n",
    "    layer1_out = relu(X_in * weights1 + biases1)\n",
    "    layer2_out = layer1_out * weights2 + biases2\n",
    "\n",
    "\n",
    "    pred_argmax = argmax(layer2_out.data, dims=2)[1][2]\n",
    "\n",
    "    if pred_argmax-1 == y_true # -1 because digits start at 0\n",
    "        global correct +=1\n",
    "    end\n",
    "    global total += 1\n",
    "\n",
    "end\n",
    "\n",
    "println(\"accuracy:\")\n",
    "println(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbc463cd-e193-4b9f-bbe3-656a7b154ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "guess_digit (generic function with 1 method)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user-friendly function\n",
    "\n",
    "function guess_digit(img_path::String)\n",
    "    \n",
    "    # load image and conver to Tensor\n",
    "    img = load(img_path)\n",
    "    img_mat = Float64.(img)\n",
    "    img_flattened = reshape(img_mat, :)\n",
    "    img_tensor = Tensor(img_flattened)\n",
    "    \n",
    "    layer1_out = relu(img_tensor * weights1 + biases1)\n",
    "    layer2_out = layer1_out * weights2 + biases2\n",
    "    pred_argmax = argmax(layer2_out.data, dims=2)[1][2]\n",
    "    prediction = pred_argmax-1 # because digits start at 0\n",
    "    \n",
    "    println(\"Guess: $prediction\")\n",
    "    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "504a4d6e-95e6-43ac-afc1-d36fa31ed1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess: 9\n"
     ]
    }
   ],
   "source": [
    "img_path = \"archive/testSet/testSet/img_3.jpg\"\n",
    "\n",
    "\n",
    "guess_digit(img_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
