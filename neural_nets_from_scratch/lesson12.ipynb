{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "223024aa-9a99-429d-84ad-6f4b58cce829",
   "metadata": {},
   "source": [
    "# Neural Nets from Scratch in Julia\n",
    "\n",
    "## Lesson 12: Adding *Tensor* functionality\n",
    "\n",
    "* In this video we'll add some additional functionality to the *Tensor* composite type.\n",
    "* [Documentation site here](https://mikesaint-antoine.github.io/SimpleGrad.jl)\n",
    "* [Github repo here](https://github.com/mikesaint-antoine/SimpleGrad.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e735d5fd-777b-44f1-91ff-5faf65b6b6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## code so far\n",
    "\n",
    "\n",
    "struct Operation{FuncType, ArgTypes}\n",
    "    op::FuncType\n",
    "    args::ArgTypes\n",
    "end\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "###### Values\n",
    "\n",
    "mutable struct Value{opType} <: Number\n",
    "    data::Float64\n",
    "    grad::Float64\n",
    "    op::opType\n",
    "end\n",
    "\n",
    "# constructor -- Value(data, grad, op)\n",
    "Value(x::Number) = Value(Float64(x), 0.0, nothing);\n",
    "\n",
    "\n",
    "import Base.show\n",
    "function show(io::IO, value::Value)\n",
    "    print(io, \"Value(\",value.data,\")\")\n",
    "end\n",
    "\n",
    "\n",
    "import Base.==\n",
    "function ==(a::Value, b::Value)\n",
    "     return a===b\n",
    "end\n",
    "\n",
    "\n",
    "import Base.+\n",
    "function +(a::Value, b::Value)\n",
    "\n",
    "    out = a.data + b.data    \n",
    "    result = Value(out, 0.0, Operation(+, (a,b))) # Value(data, grad, op)\n",
    "    return result # this should be a Value\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "backprop!(val::Value{Nothing}) = nothing\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(+), ArgTypes}\n",
    "    \n",
    "    # val = a + b\n",
    "    # update a.grad, b.grad\n",
    "    \n",
    "    val.op.args[1].grad += val.grad\n",
    "    val.op.args[2].grad += val.grad\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function backward(a::Value)\n",
    "    \n",
    "    \n",
    "    function build_topo(v::Value, visited=Value[], topo=Value[])\n",
    "    \n",
    "        if !(v in visited)\n",
    "            \n",
    "            push!(visited, v)\n",
    "            \n",
    "            if v.op != nothing\n",
    "                for operand in v.op.args\n",
    "                    \n",
    "                    if operand isa Value\n",
    "                        build_topo(operand, visited, topo)\n",
    "                    end\n",
    "                end \n",
    "            end\n",
    "            \n",
    "            push!(topo, v) \n",
    "            \n",
    "            \n",
    "        end\n",
    "        return topo\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \n",
    "    topo = build_topo(a)\n",
    "    \n",
    "    a.grad = 1\n",
    "    #da/da = 1\n",
    "    \n",
    "    for node in reverse(topo)\n",
    "        backprop!(node)\n",
    "    end\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "Base.promote_rule(::Type{<:Value}, ::Type{T}) where {T<:Number} = Value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import Base.*\n",
    "function *(a::Value, b::Value)\n",
    "\n",
    "    out = a.data * b.data    \n",
    "    result = Value(out, 0.0, Operation(*, (a,b))) # Value(data, grad, op)\n",
    "    return result # this should be a Value\n",
    " \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(*), ArgTypes}\n",
    "    \n",
    "    # val = a * b\n",
    "    # update a.grad, b.grad\n",
    "    \n",
    "    val.op.args[1].grad += val.op.args[2].data * val.grad    \n",
    "    val.op.args[2].grad += val.op.args[1].data * val.grad\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "import Base.-\n",
    "\n",
    "# negation\n",
    "function -(a::Value)\n",
    "    \n",
    "    return a * -1\n",
    "    \n",
    "end\n",
    "\n",
    "# subtraction\n",
    "function -(a::Value, b::Value)\n",
    "    \n",
    "    return a + (-b)\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base.inv\n",
    "function inv(a::Value)\n",
    "    \n",
    "    out = 1.0 / a.data\n",
    "    result = Value(out, 0.0, Operation(inv, (a,))) # Value(data, grad, op)\n",
    "    return result # this should be a Value    \n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(inv), ArgTypes}\n",
    "    \n",
    "    # val = inv(a)\n",
    "    # update a.grad\n",
    "    \n",
    "    # a.grad -= (1.0 / a.data^2) * val.grad\n",
    "    \n",
    "    val.op.args[1].grad -= (1.0 / val.op.args[1].data^2) * val.grad\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base./\n",
    "function /(a::Value, b::Value)\n",
    "     \n",
    "    # a/b = a * b^(-1)\n",
    "    \n",
    "    return a * inv(b)\n",
    "    \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "import Base.tanh\n",
    "function tanh(a::Value)\n",
    "    \n",
    "    out = (exp(2 * a.data) - 1) / (exp(2 * a.data) + 1)\n",
    "    result = Value(out, 0.0, Operation(tanh, (a,))) # Value(data, grad, op)\n",
    "    return result # this should be a Value  \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(tanh), ArgTypes}\n",
    "\n",
    "    # val = tanh(a)\n",
    "    # update a.grad\n",
    "    \n",
    "    val.op.args[1].grad += (1 - val.data^2) * val.grad\n",
    "    \n",
    "\n",
    "end\n",
    "\n",
    "####################################################################################\n",
    "###### Tensors\n",
    "\n",
    "mutable struct Tensor{opType} <: AbstractArray{Float64, 2}\n",
    "    data::Array{Float64,2}\n",
    "    grad::Array{Float64,2}\n",
    "    op::opType\n",
    "end\n",
    "\n",
    "# 2D constructor -- Tensor(data, grad, op)\n",
    "Tensor(x::Array{Float64,2}) = Tensor(x, zeros(Float64,size(x)), nothing);\n",
    "\n",
    "# 1D constructor\n",
    "function Tensor(x::Array{Float64, 1}; column_vector::Bool=false)\n",
    "\n",
    "    if column_vector\n",
    "        # reshape x to column vector - size (N,1)\n",
    "        data_2D = reshape(x, (length(x),1))\n",
    "\n",
    "    else\n",
    "        # DEFAULT - row vector - size (1,N)\n",
    "        data_2D = reshape(x, (1, length(x)))\n",
    "\n",
    "    end\n",
    "\n",
    "    return Tensor(data_2D, zeros(Float64, size(data_2D)), nothing) # Tensor(data, grad, op)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a983174-3599-4e5a-bcc1-72a054bbae57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "== (generic function with 176 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base.show\n",
    "function show(io::IO, tensor::Tensor)\n",
    "    print(io, \"Tensor(\",tensor.data,\")\")\n",
    "end\n",
    "\n",
    "backprop!(tensor::Tensor{Nothing}) = nothing\n",
    "\n",
    "\n",
    "import Base.==\n",
    "function ==(a::Tensor, b::Tensor)\n",
    "     return a===b\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e7eb30-2c31-4395-b50c-c5f64a9f4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.size(x::Tensor) = size(x.data)\n",
    "\n",
    "Base.getindex(x::Tensor, i...) = getindex(x.data, i...)\n",
    "\n",
    "Base.setindex!(x::Tensor, v, i...) = setindex!(x.data, v, i...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b95b5d3-c6ae-44df-9970-687968f0d8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([1.0 2.0; 3.0 4.0])\n",
      "(2, 2)\n",
      "Tensor([1.0 2.0; 3.0 5.0])\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([1.0 2.0; 3.0 4.0])\n",
    "\n",
    "println(x)\n",
    "\n",
    "println(size(x))\n",
    "# output: (2,2)\n",
    "\n",
    "x[2,2] = 5.0\n",
    "\n",
    "println(x)\n",
    "\n",
    "println(x[2,2,])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
